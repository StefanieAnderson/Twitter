# -*- coding: utf-8 -*-
"""
Created on Fri Aug 28 16:30:35 2015

@author: sanderson
"""

# -*- coding: utf-8 -*-
"""
Created on Thu Aug 27 16:15:20 2015

@author: Stef
"""

import pandas as pd
import numpy as np 
import seaborn as sns
from pylab import savefig
import matplotlib.pyplot as plt
import random
import sklearn as sklearn
from sklearn import preprocessing
from sklearn import linear_model, datasets
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.feature_extraction import DictVectorizer as DV
from sklearn.preprocessing import StandardScaler
from sklearn.grid_search import GridSearchCV,RandomizedSearchCV
from scipy.stats import randint, uniform
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.grid_search import GridSearchCV
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.datasets import make_hastie_10_2


train  = pd.read_csv('train.csv')

#print (train.head())


y = train.FOLLOW
train.drop('FOLLOW',axis=1, inplace=True)
train.drop('ACCOUNT_NAME',axis=1, inplace=True)
train.drop('ACCOUNT_ID',axis=1, inplace=True)
train.drop('SCREEN_NAME',axis=1, inplace=True)
train.drop('VERIFIED',axis=1, inplace=True)
train.drop('NOTIFICATIONS',axis=1, inplace=True)
train.drop('PROTECTED',axis=1, inplace=True)




X_train = train
#X_test = test

columns = train.columns
#test_ind =test.index

X_train = np.array(X_train)
#X_test = np.array(X_test)

X_train = X_train.astype(float)
#X_test = X_test.astype(float)


                        ########################
                        #                      #
                        #       Variables      #
                        #                      #
                        ########################    
      

DO_LABELENCODE_CATEGORIES = True
DO_Train_Test_Split = True
DO_LR = False
DO_LOG = False
DO_KNN = False
DO_RF = False
DO_GBR = False
DO_ADA = False


DO_SUBMIT = False



    #Labelencode


if DO_LABELENCODE_CATEGORIES:
    # label encode the categorical variables

    for i in range(X_train.shape[1]):
        if type(X_train[1,i]) is str:
            lbl = preprocessing.LabelEncoder()
            lbl.fit(list(X_train[:,i]) )            # list(X_test[:,i]))
            X_train[:,i] = lbl.transform(X_train[:,i])
            #X_test[:,i] = lbl.transform(X_test[:,i])
                
        #print ("y.shape",y.shape)
        #print ("X_train.shape",X_train.shape)
        #print ("X_test.shape new",X_test.shape)
        #print (idx.shape)

    
X_train = X_train.astype(float)
#X_test = X_test.astype(float)


   # print_Separador()


                        ########################
                        #                      #
                        #       TrainTest      #
                        #                      #
                        ########################  



if DO_Train_Test_Split:
    X_training, X_testing, y_training, y_testing = train_test_split(X_train, y, test_size=0.4)
    #print (X_training)








                        ########################
                        #                      #
                        #       Classific      #
                        #                      #
                        ########################  


    #GradientBoost
if DO_GBR:
    #X, y = make_hastie_10_2(random_state=0)
    clf = sklearn.ensemble.GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False)
    #clf = GradientBoostingRegressor(n_estimators = 100, learning_rate = 1, max_depth =2, random_state = 0, verbose = 0, min_samples_leaf=5)   
    clf.fit(X_training,y_training)
    y_pred = clf.predict(X_testing)
    print (metrics.accuracy_score(y_testing, y_pred))



    #RandomForest
if DO_RF:
    model = sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)
    model = model.fit(X_training, y_training, sample_weight=None)
    #print (metrics.accuracy_score(y_testing, y_pred))





    #KNN
if DO_KNN:

        knn = KNeighborsClassifier(n_neighbors=5)
            # passing arguments a Kneighbords: Fitmethod, feasture metrix X and responsevektor y
        knn.fit(X_training,y_training)
            # predición de una flor desconocida, espesra un numpyarray pero funciona tmb con una lista, el resultado es un array que nos da el resultado de la predicción
        y_pred = knn.predict(X_testing)
        knn.predict(X_testing)
        print(knn.predict(X_testing))
        print (metrics.accuracy_score(y_testing, y_pred))
        #print (metrics.f1_score(y_testing, y_pred))






    #LogisticRegression
if DO_LOG:
    h = .02  # step size in the mesh
    logreg = linear_model.LogisticRegression(C=1e5)
    logreg.fit(X_training,y_training)
    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    x_min, x_max = X_training[:, 0].min() - .5, X_training[:, 0].max() + .5
    y_min, y_max = X_training[:, 1].min() - .5, X_training[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])
    
    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure(1, figsize=(4, 3))
    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
    
    # Plot also the training points
    plt.scatter(X_training[:, 0], X_training[:, 1], c=y_training, edgecolors='k', cmap=plt.cm.Paired)
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.xticks(())
    plt.yticks(())
    
    plt.show()
    
    
    


    #AdaBoost
if DO_ADA:
    ada = sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)
    ada.fit(X_training,y_training)
    y_pred = ada.predict(X_testing)
    #print(ada.predict(X_testing))
    print(metrics.accuracy_score(y_testing,y_pred))


'''

    #LinearRegression
if DO_LR:

    clf = SGDClassifier(loss="hinge", penalty="l2")
    clf.fit(X_training, y_training)
    SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)
       loss="modified_huber"
  
    
 
    print (metrics.accuracy_score(y_testing, y_pred))


import numpy as np       
def Gini(y_testing, y_pred):
    # check and get number of samples
    assert y_testing.shape == y_pred.shape
    n_samples = y_testing.shape[0]
            
        # sort rows on prediction column 
        # (from largest to smallest)
    arr = np.array([y_testing, y_pred]).transpose()
    testing_order = arr[arr[:,0].argsort()][::-1,0]
    pred_order = arr[arr[:,1].argsort()][::-1,0]
        
    # get Lorenz curves
    L_testing = np.cumsum(testing_order) / np.sum(testing_order)
    L_pred = np.cumsum(pred_order) / np.sum(pred_order)
    L_ones = np.linspace(0, 1, n_samples)
        
    # get Gini coefficients (area between curves)
    G_testing = np.sum(L_ones - L_testing)
    G_pred = np.sum(L_ones - L_pred)
        
    # normalize to true Gini coefficient
    return G_pred/G_testing
    Gini(y_testing, y_pred)
    
    '''
